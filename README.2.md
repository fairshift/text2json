
# Composition of internal parser data structure — from the standpoint of one document/string which is parsed

— context (tokenized, partially waiting for additional clauses):
	Folder structure
		string
		tokens (output of filesystem.js and document.js parsing functions)
	Filename
		string, tokens
	Filemeta
		object (output of filesystem)
		tokens
	Title, Caption, Description[, and other variables provided by application container …]
		string, tokens

— document [see below]
— tokens [see below]


Document
	Identifier — hash generated by shortid.seed( ˇ ) from:
		${ url  } || 
		${ folderStructure }/${ filename } || 
		${ collection/table }/${ databaseRecord_id } || 
		customTransformation
	Name — from:
		context
		customTransformation (eg. from topmost heading)
	timestamp — from:



Token identifier
— ${documentIdentifier}: ${ ParserAbbr. }.${ ParserPckgVersion }.${ ExpressionAbbr. }( ${nonDefaultParams} )[ ${sequentialNumber} ]
— customTransformation

// Output data
Token data:
— id: ${ tokenIdentifier }
— data: // contains either a tree or a flat token (post-insertion indexing of content is more costly with SQL)
— refs:
	context: 
		${contextType}: [ 
			${ tokenIdentifier }, // external reference
			{ ${tokenIdentifier}: ${referencedTokenData} }, // for tokens which package references internally
			… 
		],
		…
	documents: [
		{ ${documentIdentifier}: [ 
			${ParserName}.${ExpressionName}(${nonDefaultParams}), // for tokens with external references
			{ ${ParserName}.${ExpressionName}(${nonDefaultParams}): ${referencedTokenData} }, // for tokens which package references internally
			…
		 … ] },
		…
	]
— index: [ // optionally, parser can be configured to create indexes in cases of both NoSQL or SQL databases)
	{ ${objectKey(root: tokenData)}:  },
	…
]
// transforms: contains a list of transforming functions (differences included or not)
// hash (cryptographic validation of inputs/outputs): 



# Expression types, expected by parser (provided by text2json package or from custom parser scripts)

## Example

Expressions — Inline
	Sentence, …

Expressions — Block
	Paragraph, …

Expressions — Tree
	List
		ListItem
			ListItem
				Paragraph
				…
			…
		…
	…


## Structure of expressions' code

Content block with a double-sided fence
	Opening sequence: fence limit, conditions
	… conditions // must match for a certain expression type
	Closing sequence: conditions, fence limit

Expression — with surrounding content scooped into token
	Opening sequence: conditions
	Opening scoop: before, after

	… conditions, scoop (before, after)

	Closing sequence: conditions
	Closing scoop: before, after



# Parser operators

Scoop
	before: 
		number of characters ||
		expressions with proximity clauses (flat, tree)

	after:
		number of characters ||
		expressions with proximity clauses (flat, tree)


Conditions — matching expressions to look for in proximity:

	${numOfSteps_negativeOrPositive}: {

		// data nested in a tree isn't parsed in this case
		flatSequence: {
			includeWhen: ${conditions},
			excludeWhen: ${conditions}
		},

		// data found in neighbour blocks
		treeSequence: {
			…
		}
	},

Condition
{
	matchIf: true || false,
	conditions: {
		regex: [rules, …],
		expr: [expr, …]
	}
},
…


# Building from examples (a recent one, of many possible)

currentObject: {
string: "MyrYesterday at 4:44 PM
What do you do with a parser package? Is it to measure something? And what do you do with node.js(i might have heard of it once) and ssr?
In any way that sounds like alot at once
oh, yes, channeling creativity hits the nail with the hammer.. though i study bioinformatics and seem stuck with the difficulty, even though i want to do it
Bbe225 | Napplauding Quaq2xToday at 3:38 AM
:)) Veela does a fine job taking in people slowly, so...
Yes, I would like to have a general implementation which parses .md, text and data trees, looking for needles in hierarchical and flat data and maybe microdata formats. Measuring is by necessity or lack thereof maybe done in comparison with markets, which either flaunt data or don't, which is proven by the consumers who type in or scan their receipts. I don't do visual computing, though. SSR is there to bring stuff quicker to the screen, Node.js does it, besides packaging JavaScript and resources.
I just made a screenshot at this point, which has enough data in filename to get back to it at a later point, by parser retrieving and outputting JSON schematized data :smiley:",

context: { filename: "Screenshot from 2019-02-07 03-39-38(Discord.gg: YOITSVEELA x @Bbe225#7232 x @Myr).png", directory__tree: "~/Pictures", needles: { hierarchical: ["%rules%"], flat: ["%matchWhen"] },

objectMap__toReduce: [
{},
{},
...
],

parsers__autodetect: {
...dontParseSameFileTwice()
} }

## Parser config schema conceptualization (expressions, simplified)

export { invoke, config, parser }


/*

Start-up: initializing parser

*/

const invoke = {
  sl_SL: {
    'formal:_': [
  	 { s: 'lep*' },
     { s: 'gest+' }
    ],
    'prekmurščina:|': [
      {s: 'le*p+' },
      { s: 'ge*st+' }

    ]
  },
  en_EN: {
    'leap:|': [
      { s: ['Leap'], caseSensitive: true },
      { s: ['gesture+'] }
    ],
    'fine:3': [
      { l: ['fine', 'gesture+'] },
      { x: ['don\'t parse'] },
      { s: ['gesture+'] }
    ]
  },
  '*': {
    '#leapgest:|': [
      { s: ['#leapgest*'], '*_': '[a-z]'}
    ]
  }
// nearby: [ invokeConfig.nearby.flat, invokeConfig.nearby.tree ], // [-]
}

const config = {
  caseSensitive: false,
  nearby: { 
    flat: [-1, 1],
    left: [-1, 1]
  },
  charReplacements: {
    '*_': '[Aa-Zz]',
    '+_': '[Aa-Zz]'
  }
  parseText: { plugin: "markdown-it" }
}
