

export { customParser }




const customParser = (name, categoryList) => { 
// 	… as a wrapping/injected and controlling module of text2json parser pckg
//	 (delivering partial renders of inputs to text2json from/to a number of sources)

	//
	// Flows of mapping are loaded …
	// — Parser name mapping 								[« !!!]
	// — Filename & directory path [_…] structure mapping 	[« !!!]
	// — External data import (validate against JSON schema)
	// … script loads XML/React components for these containers of …
	// … various scenarios — loaded in expressions' Trie object


	//
	// Parse document with context

	// … JSON schema is imported from various required files, from …
	// 	 main parser package ; custom parser package ; 

	/*	
		Find
		— expressions in a tree
		… compare with a document's flow

	
		Transform
		… and mappings are used to nest data into chains of tokens (recognizable by/as a cached index)


		Tokenize
		… tree structures of tokens are formed
		… tracks relations among data nodes' counterparts


		Parser-contained database tasks are run — script …				
		… creates objects
		… meta data is appended with custom keys, to the same "db" 
		database object (in local storage of script run)
		

		External augmenting scripts are run:
		— aggregation and anonymization mappings [{transformFunctions_evaluated}]
		— rhizome knots/mappings [{connotationList: StringType}]		
		— validation: checksums / hashes
		

		Data nodes are tied to a database structure (noSql or MySql)

		
		» There's an end to a locality, data is sent to a remote client, databaseTasks can be run then
		 (a transaction log may be exported, along, as a blockchain type of data structure, 
		  trees of functions evaluating data and returning signatures of content in parts)

	*/
}